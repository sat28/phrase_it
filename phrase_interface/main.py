import pandas as pd
import chardet
import gensim
from gensim.models import KeyedVectors
import re
import numpy as np
from operator import add
from scipy.spatial.distance import cosine

def wv_mapping(row, wv):
    """
    Maps each word of phrase to word2vec model value
    :param row: row containing each phrase
    :param wv: the word2vec model
    :return: mapped words i.e. array for each word from wv
    """
    row_mapping = []
    for word in row.split():
        word = re.sub('[^A-Za-z0-9]+', '', word) # remove special chars like ?
        try:
            row_mapping.append(wv.get_vector(word.lower()))
        except:
            # this looks after stop words and for any word that is not in our wv model
            # we add a null array for such case
            row_mapping.append([0] * wv.vector_size)
    return row_mapping


def normalise(row, wv):
    normalised_row = [0] * wv.vector_size
    # adds all the wv mapped rows
    for word_mapping in row:
        normalised_row = list(map(add, normalised_row, word_mapping))

    # normalises the sum
    norm = np.linalg.norm(normalised_row)
    normal_array = normalised_row / norm
    return normal_array


class Phrase:
    def __init__(self):
        with open('data/phrases.csv', 'rb') as f:
            result = chardet.detect(f.read())
        # store the phrases in a pandas df
        self.phrases = pd.read_csv('data/phrases.csv', encoding=result['encoding'])
        self.wv = None
        self.phrases_normalised = None
        self.cosine_matrix = None

    def read_model(self):
        """
        directly reading the model generated by word2vec (https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit)
        :return: word2vec model stored with values for each word in google news dict
        """
        self.wv = KeyedVectors.load('data/vectors.pickle')

    def batch_normalise(self):
        """
        apply word2vec mapping to each word of each phrase and then add all mappings and normalise
        :return: normalised array for each phrase produced
        """
        self.phrases_normalised = self.phrases['Phrases'].apply(wv_mapping, wv=self.wv).apply(normalise, wv=self.wv)

    def batch_cosine(self):
        """
        do batch cosine similarity of all phrases by all others
        :return: cosine similarity matrix : a pandas df for each phrase
        """
        self.cosine_matrix = pd.DataFrame()
        for index, row in self.phrases_normalised.iteritems():
            similarities = []
            for rem_row in self.phrases_normalised:
                similarities.append(1 - cosine(row, rem_row))
            self.cosine_matrix[str(index)] = similarities

    def similar_phrase(self, single_phrase):
        """
        Finds the most similiar phrase and the cosine value
        :param single_phrase: input from user
        :return: the matched phrase and cosine similarity value
        """
        wv_value = pd.Series(single_phrase).apply(wv_mapping, wv=self.wv).apply(normalise, wv=self.wv)
        max_simlarity = 0
        similiar_row = ''
        for index, row in self.phrases_normalised.iteritems():
            similarity = 1 - cosine(row, wv_value[0])
            if similarity > max_simlarity:
                max_simlarity = similarity
                similiar_row = self.phrases.iloc[index]
        return similiar_row['Phrases'], round(max_simlarity, 2)
