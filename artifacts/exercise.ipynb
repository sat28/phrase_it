{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import chardet\n",
    "import pandas as pd\n",
    "\n",
    "with open('/Users/shaleentaneja/Downloads/Senior Data Engineer/phrases.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())  # or readline if the file is large\n",
    "\n",
    "\n",
    "phrases = pd.read_csv('/Users/shaleentaneja/Downloads/Senior Data Engineer/phrases.csv', encoding=result['encoding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phrases    what are the total losses for companies in cou...\n",
       "Name: 4, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/shaleentaneja/opt/anaconda3/lib/python3.8/site-packages (4.0.1)\n",
      "Requirement already satisfied: nltk in /Users/shaleentaneja/opt/anaconda3/lib/python3.8/site-packages (3.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/shaleentaneja/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.5.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/shaleentaneja/opt/anaconda3/lib/python3.8/site-packages (from gensim) (5.0.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Users/shaleentaneja/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.18.5)\n",
      "Requirement already satisfied: tqdm in /Users/shaleentaneja/opt/anaconda3/lib/python3.8/site-packages (from nltk) (4.47.0)\n",
      "Requirement already satisfied: click in /Users/shaleentaneja/opt/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in /Users/shaleentaneja/opt/anaconda3/lib/python3.8/site-packages (from nltk) (2020.6.8)\n",
      "Requirement already satisfied: joblib in /Users/shaleentaneja/opt/anaconda3/lib/python3.8/site-packages (from nltk) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shaleentaneja/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load_word2vec_format(\"/Users/shaleentaneja/Downloads/GoogleNews-vectors-negative300.bin.gz\", binary=True, limit=1000000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.save('vectors.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load('vectors.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shaleentaneja/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from operator import add \n",
    "\n",
    "def wv_mapping(row, wv):\n",
    "    row_mapping = []\n",
    "    for word in row.split():\n",
    "        word = re.sub('[^A-Za-z0-9]+', '', word)\n",
    "        try:\n",
    "            row_mapping.append(wv.get_vector(word.lower()))\n",
    "        except:\n",
    "            row_mapping.append([0] * wv.vector_size)\n",
    "    return row_mapping\n",
    "\n",
    "def normalise(row, wv):\n",
    "    normalised_row = [0] * wv.vector_size\n",
    "    for word_mapping in row:\n",
    "        normalised_row = list(map(add, normalised_row, word_mapping))\n",
    "    norm = np.linalg.norm(normalised_row)\n",
    "    normal_array = normalised_row/norm\n",
    "    return normal_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phrases_normalised = phrases['Phrases'].apply(wv_mapping, wv=wv).apply(normalise, wv=wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cosine_matrix = pd.DataFrame()\n",
    "for index, row in phrases_normalised.iteritems():\n",
    "    similarities = []\n",
    "    for rem_row in phrases_normalised:\n",
    "        similarities.append(1 - cosine(row, rem_row))\n",
    "    cosine_matrix[index] = similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_phrase(phrase):\n",
    "    wv_value = pd.Series(phrase).apply(wv_mapping, wv=wv).apply(normalise, wv=wv)\n",
    "    max_simlarity = 0\n",
    "    similiar_row = ''\n",
    "    for index, row in phrases_normalised.iteritems():\n",
    "        similarity = 1 - cosine(row, wv_value[0])\n",
    "        if similarity > max_simlarity:\n",
    "            max_simlarity = similarity\n",
    "            similiar_row = phrases.iloc[index]\n",
    "    return similiar_row, max_simlarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Phrases    What are the biggest acquisitions in 2020?\n",
       " Name: 47, dtype: object,\n",
       " 0.695068245280913)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_phrase('acquisitions in 2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
